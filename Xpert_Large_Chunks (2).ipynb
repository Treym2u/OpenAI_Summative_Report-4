{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8835316-7f53-49f2-973d-f5ec6cf634ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import HTML, display, clear_output, Javascript\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive, fixed, Button, VBox, HBox\n",
    "from ipywidgets import interact, widgets\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "import spacy\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "# from Upcoming_Modules import syllabus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5794d47e-fe0c-4f6c-a224-7fa5a4a844f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad63cf2c-6c12-4a15-b970-0754821dcfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb84a99c-3fba-42ac-8c82-97e7dbbb5494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "# Detect the encoding of the CSV file\n",
    "with open('Message Log.csv', 'rb') as file:\n",
    "    result = chardet.detect(file.read())\n",
    "\n",
    "encoding_type = result['encoding']\n",
    "\n",
    "# Read the CSV file with detected encoding and tab separator\n",
    "df = pd.read_csv('Message Log.csv', encoding=encoding_type, delimiter=\"\\t\", engine='python')\n",
    "\n",
    "# If needed, strip timezone information\n",
    "df['Message Timestamp'] = df['Message Timestamp'].str.split('+').str[0]\n",
    "\n",
    "# Ensure that 'Message Timestamp' is in datetime format\n",
    "df['Message Timestamp'] = pd.to_datetime(df['Message Timestamp'], format='%m/%d/%Y', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543b5413-4e5b-4e14-aa6f-d3d597e686d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique cohort names and sort them alphabetically\n",
    "unique_cohorts = sorted([x for x in df['Class Name'].unique() if isinstance(x, str)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18afcf6d-d1ce-4b56-804a-bf32cd0f005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-message rows\n",
    "df = df[df['Message'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c31f8e07-cd34-4e98-891e-686eda4702b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming a 'messages' column that contains chat inputs\n",
    "df['Message_Clean'] = df['Message'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9adfbaf4-593c-439c-b190-9da058705ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your date column is in datetime format\n",
    "df['Message Timestamp'] = pd.to_datetime(df['Message Timestamp'], format='mixed', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2985690b-8c58-4af6-a9d5-dc381af16a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def large_chunk_text(text, max_length=128000, prompt_length=0):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "        # Tokenize the text using spaCy\n",
    "        doc = nlp(text, disable=[\"parser\", \"ner\", \"tagger\"])  # Only tokenization is needed\n",
    "\n",
    "    max_tokens_per_chunk = max_length - prompt_length - 1\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_chunk_token_count = 0\n",
    "\n",
    "    for token in doc:\n",
    "        token_text_with_ws = token.text_with_ws\n",
    "        if current_chunk_token_count + len(token_text_with_ws) > max_tokens_per_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = \"\"\n",
    "            current_chunk_token_count = 0\n",
    "\n",
    "        current_chunk += token_text_with_ws\n",
    "        current_chunk_token_count += len(token_text_with_ws)\n",
    "\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks, current_chunk_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238dfbf9-a626-4828-9434-fbee6835bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_length=1280000, buffer=10000):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "        # Tokenize the text using spaCy\n",
    "        doc = nlp(text, disable=[\"parser\", \"ner\", \"tagger\"])  # Only tokenization is needed\n",
    "        tokens = [token.text for token in doc]\n",
    "\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Setting a maximum chunk size with a conservative buffer\n",
    "    max_chunk_size = max_length - buffer  # Reducing chunk size to account for tokenization differences\n",
    "\n",
    "    chunks = []\n",
    "    start_index = 0\n",
    "    total_tokens_processed = 0\n",
    "    while start_index < token_count:\n",
    "        end_index = start_index + max_chunk_size\n",
    "\n",
    "        # Adjust end_index if it exceeds the total number of tokens\n",
    "        if end_index > token_count:\n",
    "            end_index = token_count\n",
    "\n",
    "        # Combine tokens back to text for the chunk\n",
    "        chunk = ' '.join(tokens[start_index:end_index])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        total_tokens_processed += (end_index - start_index)\n",
    "        if total_tokens_processed > max_length:\n",
    "            raise ValueError(\"Total token count exceeds the maximum allowed limit. Consider reducing the input size.\")\n",
    "\n",
    "        start_index = end_index\n",
    "\n",
    "    return chunks, total_tokens_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0891ffa7-0fb8-4ee3-ac23-b37b1439c5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_check(text, max_length=1280000, buffer=10000):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "        # Tokenize the text using spaCy\n",
    "        doc = nlp(text, disable=[\"parser\", \"ner\", \"tagger\"])  # Only tokenization is needed\n",
    "        tokens = [token.text for token in doc]\n",
    "\n",
    "    token_count = len(tokens)\n",
    "    if token_count > 100000:\n",
    "        return large_chunk_text(text)\n",
    "        \n",
    "    else:\n",
    "        return chunk_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbdcc5a-e2a5-4dec-9757-214bdb3cf079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(text):\n",
    "    summaries = []\n",
    "    \n",
    "    # Chunk the input text\n",
    "    # chunks, token_count = chunk_text(text)\n",
    "    chunks, token_count = size_check(text)\n",
    "  \n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            selected_prompt = prompt_dropdown.value\n",
    "            if selected_prompt == \"Custom\":\n",
    "                prompt_text = custom_prompt_textarea.value\n",
    "            else:\n",
    "                prompt_text = prompt_options[selected_prompt]\n",
    "\n",
    "            prompt_text = prompt_text.format(text=chunk)  # Use the chunk here\n",
    "\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt_text}\n",
    "                ],\n",
    "                max_tokens=4096  # This is the maximum output tokens, not input\n",
    "            )\n",
    "            summary = response['choices'][0]['message']['content'].strip()\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            summaries.append(None)\n",
    "    \n",
    "    # Join the summaries from each chunk\n",
    "    return \" \".join([s for s in summaries if s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b882530-005c-46ae-bd1c-3065af8eabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(cohort_name, start_date, end_date):\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=False)\n",
    "    \n",
    "    # Set the API key at the beginning of the function\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    openai.api_key = api_key\n",
    "    \n",
    "    # Check if the cohort_name exists in the DataFrame\n",
    "    if cohort_name not in df['Class Name'].unique():\n",
    "        display(HTML('''\n",
    "        <div style=\"color: red; font-size: 20px; text-align: center; margin-top: 50px;\">\n",
    "            Error: The provided cohort name does not exist in the dataset.\n",
    "        </div>\n",
    "        '''))\n",
    "        return\n",
    "    \n",
    "    # Convert strings to datetime\n",
    "    start_date_dt = pd.Timestamp(start_date)\n",
    "    end_date_dt = pd.Timestamp(end_date)\n",
    "    \n",
    "    # Check if the date range exists in the DataFrame\n",
    "    if not ((df['Message Timestamp'] >= start_date_dt) & (df['Message Timestamp'] <= end_date_dt)).any():\n",
    "        display(HTML('''\n",
    "        <div style=\"color: red; font-size: 20px; text-align: center; margin-top: 50px;\">\n",
    "            Error: The provided date range does not exist in the dataset.\n",
    "        </div>\n",
    "        '''))\n",
    "        return\n",
    "    \n",
    "    # Simulate a loading time with a progress message in orange\n",
    "    display(HTML('''\n",
    "    <div style=\"color: orange; font-size: 20px; text-align: center; margin-top: 50px;\">\n",
    "        Processing data...\n",
    "    </div>\n",
    "    '''))\n",
    "    \n",
    "    # Apply summarization to chat segments\n",
    "    cohort = df[df['Class Name'] == cohort_name]\n",
    "    date_range = cohort[(cohort['Message Timestamp'] >= start_date_dt) & (cohort['Message Timestamp'] <= end_date_dt)]\n",
    "    user = date_range[(date_range['User Role'] == 'user') & (date_range['Conversation ID.1'].notnull())]\n",
    "    \n",
    "    # Concatenate all messages into a single string\n",
    "    all_messages = ' '.join(user['Message'].astype(str))\n",
    "    \n",
    "    # Chunk the text if it exceeds the maximum token limit\n",
    "    chunks, token_count = chunk_text(all_messages)\n",
    "    if token_count > 100000:\n",
    "        print('running get_summary')\n",
    "\n",
    "    # Display the token count\n",
    "    display(HTML(f'<div style=\"color: blue; font-size: 20px; text-align: center; margin-top: 20px;\">Token Count: {token_count} tokens</div>'))\n",
    "    if len(chunks) > 1:\n",
    "        display(HTML(f'''\n",
    "        <div style=\"color: blue; font-size: 20px; text-align: center; margin-top: 50px;\">\n",
    "            Chunking initiated due to content length. Total chunks: {len(chunks)}\n",
    "        </div>\n",
    "        '''))\n",
    "\n",
    "    if not all_messages:\n",
    "        print(\"No messages to process.\")\n",
    "        return\n",
    "    \n",
    "    # Get summary of all messages\n",
    "    summaries = [get_summary(chunk) for chunk in chunks]\n",
    "  \n",
    "    summary = \"\\n\\n\".join(summaries)\n",
    "    if token_count > 100000:\n",
    "        summary = get_summary(summary)\n",
    "        print('running get_summary')\n",
    "\n",
    "    if summary is None:\n",
    "        print(\"Failed to generate a summary. Please check the get_summary function.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate user message counts for the specific cohort and time frame\n",
    "    user_message_counts = user['User ID'].value_counts()\n",
    "    user_message_count_str = \"\\n\".join([f\"User {uid}: {count} messages\" for uid, count in user_message_counts.items()])\n",
    "    \n",
    "    # Create a dynamic filename based on the cohort name and date range\n",
    "    filename = f'{cohort_name}_{start_date_dt.strftime(\"%Y%m%d\")}_to_{end_date_dt.strftime(\"%Y%m%d\")}.txt'.replace(\":\", \"\").replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    \n",
    "    # Check if a folder with the cohort name exists, if not, create it\n",
    "    folder_name = cohort_name.replace(\":\", \"\").replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    \n",
    "    # Save summary to a text file inside the cohort folder\n",
    "    file_path = os.path.join(folder_name, filename)\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(summary)\n",
    "        # Add a separator for clarity\n",
    "        file.write(\"\\n\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "        # Write the header for user message counts\n",
    "        file.write(\"User Message Counts:\\n\")\n",
    "\n",
    "        # Format and write each user's message count\n",
    "        for uid, count in user_message_counts.items():\n",
    "            file.write(f\"    User {uid}: {count} messages\\n\")\n",
    "    \n",
    "    display(HTML(f'''\n",
    "        <div style=\"\n",
    "            border: 2px solid #4CAF50;\n",
    "            padding: 10px;\n",
    "            margin: 5px 0;\n",
    "            border-radius: 5px;\n",
    "            background-color: #f9f9f9;\n",
    "            text-align: center;\n",
    "        \">\n",
    "            <a href=\"{file_path}\" target=\"_blank\" style=\"\n",
    "                text-decoration: none;\n",
    "                color: #4CAF50;\n",
    "                font-weight: bold;\n",
    "                font-size: 16px;\n",
    "            \">\n",
    "                Click here to view {filename}\n",
    "            </a>\n",
    "        </div>\n",
    "        '''))\n",
    "    \n",
    "    # Extracting the selected prompt\n",
    "    selected_prompt = prompt_dropdown.value\n",
    "    \n",
    "    # At the end, display success message in green\n",
    "    display(HTML(f'''\n",
    "    <div style=\"color: green; font-size: 20px; text-align: center; margin-top: 50px;\">\n",
    "        Data processed successfully!\n",
    "    </div>\n",
    "    <div style=\"color: black; font-size: 18px; text-align: center; margin-top: 20px;\">\n",
    "        Cohort Analyzed: {cohort_name}\n",
    "    </div>\n",
    "    <div style=\"color: black; font-size: 18px; text-align: center; margin-top: 10px;\">\n",
    "        Time Stamp Analyzed: From {start_date} to {end_date}\n",
    "    </div>\n",
    "    <div style=\"color: black; font-size: 18px; text-align: center; margin-top: 10px;\">\n",
    "        Words Processed: {len(all_messages.split())}\n",
    "    </div>\n",
    "    <div style=\"color: black; font-size: 18px; text-align: center; margin-top: 10px;\">\n",
    "        Prompt Used: {selected_prompt}\n",
    "    </div>\n",
    "    '''))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14f100e2-a4e2-4be3-a8d1-d31a8925e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the widgets and interactive function\n",
    "cohort_dropdown = widgets.Dropdown(\n",
    "    options=unique_cohorts,\n",
    "    description='Cohort:',\n",
    "    disabled=False,\n",
    ")\n",
    "start_date_widget = widgets.DatePicker(description='Start Date', value=pd.to_datetime(''))\n",
    "end_date_widget = widgets.DatePicker(description='End Date', value=pd.to_datetime(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2101deb6-8f7f-4747-9e14-a59751487973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dropdown widget to select a cohort\n",
    "cohort_dropdown = widgets.Dropdown(\n",
    "    options=unique_cohorts,\n",
    "    description='Cohort:',\n",
    "    disabled=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01018888-8d41-4943-b175-abd02536a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the default prompt\n",
    "default_prompt = (\"Please analyze the student comments and questions in the text: {text}. Provide a high level summary of the comments and questions, a list of topics students are struggling with and insights for the instructor.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72a299dc-3325-47ea-8777-9481c299622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_options = {\n",
    "    \"Default Prompt\": default_prompt\n",
    "}\n",
    "\n",
    "   \n",
    "prompt_dropdown = widgets.Dropdown(\n",
    "    options=prompt_options.keys(),\n",
    "    value=\"Default Prompt\",\n",
    "    description='Prompt:'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49561dfd-508c-4baa-aac1-9a20ec00899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_all(b):\n",
    "    cohort_dropdown.value = None  # Set to None or the first option\n",
    "    start_date_widget.value = None\n",
    "    end_date_widget.value = None\n",
    "    if prompt_options and isinstance(prompt_options, list):  # Check if it's a list and not empty\n",
    "        prompt_dropdown.value = prompt_options[0]\n",
    "    elif prompt_options and isinstance(prompt_options, dict):  # Check if it's a dictionary and not empty\n",
    "        prompt_dropdown.value = next(iter(prompt_options.keys()))  # Set to the first key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b6e3ec5-348e-4286-9fb5-8f476373d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display all widgets to the user\n",
    "def display_ui():\n",
    "    # For processing data\n",
    "    process_data_button = widgets.Button(description=\"Process Data\")\n",
    "    clear_button = widgets.Button(description=\"Clear Input\")\n",
    "    process_data_button.on_click(lambda b: process_data(cohort_dropdown.value, start_date_widget.value, end_date_widget.value))\n",
    "    clear_button.on_click(clear_all)\n",
    "    \n",
    "\n",
    "    # Adjusting the width of the dropdown\n",
    "    cohort_dropdown.layout.width = '500px'  # Adjust the width as per your preference\n",
    "\n",
    "\n",
    "    # Display all widgets, including those for processing data and saving module content\n",
    "    display(VBox([\n",
    "        # HBox([prompt_dropdown, custom_prompt_textarea]),\n",
    "        VBox([cohort_dropdown, start_date_widget, end_date_widget]),\n",
    "        HBox([process_data_button, clear_button]),\n",
    "    ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6187b5d5-34e6-4ce0-9a88-6ab52fdf8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the widget using your unique_cohorts list\n",
    "cohort_dropdown = widgets.Dropdown(\n",
    "    options=unique_cohorts,\n",
    "    description='Cohort:',\n",
    "    disabled=False,\n",
    ")\n",
    "# interact(display_current_unit, cohort=cohort_dropdown);\n",
    "interact(cohort=cohort_dropdown);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c38137b-f196-4fc8-ad47-2d6fa248edf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d1d976939948d28504a406c1af10c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Dropdown(description='Cohort:', layout=Layout(width='500px'), options=('ASU-VIRTâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the UI\n",
    "display_ui()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
